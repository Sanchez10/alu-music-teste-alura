# Teste vaga de Python com foco em LLM | Alura

OlÃ¡! Que bom ter vocÃª por aquiâ€¯ğŸš€

Alguns pontos estÃ£o propositadamente abertos para avaliarmos seu raciocÃ­nio. Anote qualquer dÃºvida e registre as decisÃµes de arquitetura que tomar.

---

## Tecnologias **obrigatÃ³rias**

| Camada       | Requisitos                                                      |
| ------------ | --------------------------------------------------------------- |
| ExecuÃ§Ã£o     | Pythonâ€¯â‰¥â€¯3.10 Â â€¢Â  Docker                                        |
| Web          | Flask                                                           |
| PersistÃªncia | SQL (MySQL ou PostgreSQL)                                       |
| LLM          | modeloâ€¯Ã  sua escolha                                            |
| Testes       | PyTest cobrindo unidades **e** integraÃ§Ãµes                      |
| CI/CD        | Gitâ€¯+â€¯GitHub â€¢ GitHubÂ Actions **acionado por pipeline Jenkins** |

> O pipeline do Jenkins deve iniciar o workflow do GitHubÂ Actions e exibir o resultado â€” demonstre isso no Jenkinsfile e no README.

---

## AvaliaÃ§Ã£o

| Eixo                        | O que observaremos                                               |
| --------------------------- | ---------------------------------------------------------------- |
| **Qualidade tÃ©cnica**       | arquitetura, clareza de cÃ³digo, testes, paralelismo              |
| **DerivaÃ§Ã£o de requisitos** | hipÃ³teses registradas, justificativas                            |
| **OrganizaÃ§Ã£o**             | estrutura de pastas, granularidade de commits, documentaÃ§Ã£o      |
| **Devâ€¯Experience**          | ambiente Docker simples de subir, CI verde, comandos utilitÃ¡rios |
| **Evals**                   | mÃ©tricas automatizadas que mostrem a qualidade da classificaÃ§Ã£o  |

---

## Entrega

1. RepositÃ³rio no GitHub (pÃºblico ou privado). Se privado, adicione:
   - [lucasboot](https://github.com/lucasboot)
2. **README** explicando:
   - Como levantar tudo via Docker
   - Como rodar testes e evals
   - Como acompanhar cada etapa dos pipelines
   - Principais decisÃµes de design

---

# Sobre o desafio

## CenÃ¡rio

A **AluMusic** acompanha milhares de comentÃ¡rios deixados por ouvintes sobre artistas, Ã¡lbuns, clipes e shows. O time de curadoria musical precisa de sinais rÃ¡pidos sobre o que estÃ¡ gerando hype, insatisfaÃ§Ã£o ou dÃºvidas no pÃºblico.

Sua missÃ£o Ã© criar um serviÃ§o que **receba, classifique e analise** esses comentÃ¡rios, oferecendo:

1.â€¯um endpoint autenticado para ingestÃ£o;

2.â€¯um painel privado para a equipe de curadoria;

3.â€¯um relatÃ³rio pÃºblico em tempo real;

4.â€¯um resumo semanal automatizado por eâ€‘mail.

---

### 1.â€¯ClassificaÃ§Ã£o de ComentÃ¡rios

- **Entrada**: `id` (UUID) + `texto`, enviados individualmente ou em lote.
- **SaÃ­da**:
  - `categoria` âˆˆ { **ELOGIO**, **CRÃTICA**, **SUGESTÃƒO**, **DÃšVIDA**, **SPAM** }
  - `tags_funcionalidades` (lista: cÃ³digo + explicaÃ§Ã£o â€” ex.: `feat_autotune`, `clip_narrativa`, `show_duraÃ§Ã£o`)
  - `confianca` (0â€‘1)
- Deve haver autenticaÃ§Ã£o (JWT, sessÃ£o, etc.) no endpoint `/api/comentarios`.
- Lotes grandes devem ser processados em paralelo.

---

### 2.â€¯RelatÃ³rio em tempo real

Rota pÃºblica `/relatorio/semana` devolvendo **cinco grÃ¡ficos obrigatÃ³rios** (HTML ou imagem embutida) e/ou JSON correspondente, atualizados no mÃ¡ximo a cada 60â€¯s. Os grÃ¡ficos devem trazer insights relevantes â€” por exemplo:

- Categorias mais frequentes por artista
- EvoluÃ§Ã£o de crÃ­ticas apÃ³s lanÃ§amento
- Tags mais citadas nas Ãºltimas 48h

---

### 3.â€¯Dashboard privado

Interface com login para:

- Pesquisar e filtrar comentÃ¡rios por artista, Ã¡lbum, tag ou categoria
- Ver histÃ³rico de classificaÃ§Ãµes
- Exportar dados

---

### 4.â€¯Resumo semanal por eâ€‘mail

Ao final de cada semana:

1. Gerar texto com LLM destacando tendÃªncias
2. Enviar automaticamente aos stakeholders
3. Armazenar o resumo na base de dados

---

### 5.â€¯Evals & MÃ©tricas

- Defina critÃ©rios de avaliaÃ§Ã£o do modelo de classificaÃ§Ã£o (recall para SPAM, por exemplo).
- Deixe um comando Ãºnico que rode os evals e gere um relatÃ³rio de mÃ©tricas. O CI deve falhar se os resultados ficarem abaixo de um patamar mÃ­nimo definido por vocÃª.

---

### 6.â€¯Extra (nÃ£o obrigatÃ³rio)

**Miniâ€¯Insightâ€‘Q&A**

Adicione uma rota autenticada `/insights/perguntar` (HTTPâ€¯POST) onde stakeholders possam enviar **uma pergunta curta em linguagem natural** sobre os feedbacks das **Ãºltimas 8â€¯semanas**.

**Fluxo mÃ­nimo**

1. Ao receber a pergunta, a aplicaÃ§Ã£o seleciona os **trÃªs resumos semanais** mais recentes.
2. Concatena esses resumos como contexto e chama a LLM para responder em atÃ© **150â€¯palavras**.
3. A resposta deve conter:
   - Texto gerado pela LLM.
   - Lista das semanas citadas como fonte (ex.: `[â€œ2025â€‘W30â€, â€œ2025â€‘W29â€]`).
4. Retorne tudo em JSON.

---

Boa sorte! Estamos ansiosos para ver como vocÃª vai evoluir este desafioâ€¯ğŸ”¥
